{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***TUTORIAL 6***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Enk9Du-_SBIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Libraries\n"
      ],
      "metadata": {
        "id": "rqeiRc0rTmOE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPzkcl_URRnv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_vgg\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as preprocess_resnet\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load and Preprocess the Image"
      ],
      "metadata": {
        "id": "EFL83WH0TzS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_image(img_path, model_name):\n",
        "    # Load the image with target size (224, 224)\n",
        "    img = image.load_img(img_path, target_size=(224, 224))  # Resize to 224x224 pixels\n",
        "    img_array = image.img_to_array(img)  # Convert to numpy array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    if model_name == 'vgg':\n",
        "        # Preprocess image according to the model's requirements\n",
        "        img_array = preprocess_vgg(img_array)\n",
        "    elif model_name == 'resnet':\n",
        "        img_array = preprocess_resnet(img_array)\n",
        "\n",
        "    return img_array"
      ],
      "metadata": {
        "id": "n2_qMjx_T19e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load Pretrained Models"
      ],
      "metadata": {
        "id": "whIyxPJvT37o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "\n",
        "vgg_model = VGG16(weights='imagenet')\n",
        "resnet_model = ResNet50(weights='imagenet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOgq1NERT6Qc",
        "outputId": "20243136-6343-4a65-c647-7c4e3e4ae545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Make Predictions"
      ],
      "metadata": {
        "id": "_QrOL0a2T8Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import decode_predictions as decode_vgg\n",
        "from tensorflow.keras.applications.resnet50 import decode_predictions as decode_resnet\n",
        "\n",
        "def make_top_5_predictions(model, img_array, model_name):\n",
        "    # Make predictions\n",
        "    if model_name == 'vgg':\n",
        "        preds = model.predict(img_array)\n",
        "        return decode_vgg(preds, top=5)  # decode predictions (VGG format)\n",
        "    elif model_name == 'resnet':\n",
        "        preds = model.predict(img_array)\n",
        "        return decode_resnet(preds, top=5)  # decode predictions (Resnet format)\n",
        "\n",
        "def print_top_5_predictions(model, img_path, model_name):\n",
        "    img_array = load_and_preprocess_image(img_path, model_name)  # Load and preprocess image\n",
        "    predictions = make_top_5_predictions(model, img_array, model_name)  # get top 5 predictions\n",
        "    print(f\"Top 5 predictions with probabilities for {model_name.upper()}:\")\n",
        "    for i, (imagenet_id, label, prob) in enumerate(predictions[0]):\n",
        "        print(f\"{i+1}. {label} ({prob*100:.2f}% probability)\")"
      ],
      "metadata": {
        "id": "Bv2D9f7kULsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Define Image Path"
      ],
      "metadata": {
        "id": "Wfxrk6Z1T8CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your image\n",
        "#img_path = r\"D:\\Deep Learning Course\\Codes\\Images for pretraining\\Dog.jpg\"\n",
        "\n",
        "img_path = r\"/cat.png\""
      ],
      "metadata": {
        "id": "anhoPuczUodV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Get Predictions from VGG16 and ResNet50"
      ],
      "metadata": {
        "id": "wHPYoHh7UrIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict top 5 using VGG16\n",
        "vgg_img_array = load_and_preprocess_image(img_path, 'vgg')\n",
        "vgg_predictions = make_top_5_predictions(vgg_model, vgg_img_array, 'vgg')\n",
        "print(\"Top 5 predictions from VGG16:\")\n",
        "for i, pred in enumerate(vgg_predictions[0]):\n",
        "    print(f\"{i+1}. {pred[1]} ({pred[2]*100:.2f}% probability)\")\n",
        "\n",
        "# Predict top 5 using Resnet50\n",
        "resnet_img_array = load_and_preprocess_image(img_path, 'resnet')\n",
        "resnet_predictions = make_top_5_predictions(resnet_model, resnet_img_array, 'resnet')\n",
        "print(\"Top 5 predictions from ResNet50:\")\n",
        "for i, pred in enumerate(resnet_predictions[0]):\n",
        "    print(f\"{i+1}. {pred[1]} ({pred[2]*100:.2f}% probability)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL5OelQDUs-X",
        "outputId": "d8d239c3-2406-4405-aa38-7dc13a7f33f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Top 5 predictions from VGG16:\n",
            "1. golden_retriever (73.44% probability)\n",
            "2. otterhound (10.59% probability)\n",
            "3. Afghan_hound (6.84% probability)\n",
            "4. kuvasz (2.69% probability)\n",
            "5. English_setter (1.06% probability)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Top 5 predictions from ResNet50:\n",
            "1. otterhound (55.41% probability)\n",
            "2. Afghan_hound (16.62% probability)\n",
            "3. golden_retriever (11.04% probability)\n",
            "4. standard_poodle (10.70% probability)\n",
            "5. bloodhound (1.66% probability)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 1: Trying Different Architectures***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B_0Hygjt0Dik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making AlexNet architecture from scratch while importing ResNet01 & MobileNet.\n",
        "I gave a cat image to all models as input and the result showed Alexnet & MobileNet probability faster in 0sec while ResNet101 took 4sec. ResNet & MobileNet achieved higher accuracy while Alexnet showed mostly wrong probabilities.\n"
      ],
      "metadata": {
        "id": "1C3CmZBUJo5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg, decode_predictions as decode_vgg\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50, decode_predictions as decode_resnet50\n",
        "from tensorflow.keras.applications.resnet import ResNet101, preprocess_input as preprocess_resnet101, decode_predictions as decode_resnet101\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input as preprocess_mobile, decode_predictions as decode_mobile\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "import numpy as np\n",
        "\n",
        "'''Custom AlexNet model'''\n",
        "def create_alexnet():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), padding='valid', activation='relu', input_shape=(224,224,3)))\n",
        "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(5,5), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1000, activation='softmax'))  # For ImageNet classes\n",
        "    return model\n",
        "\n",
        "#                              Loading and Preprocessing the Image\n",
        "def load_and_preprocess_image(img_path, model_name):\n",
        "    target_size = (224, 224) if model_name != 'mobile' else (224, 224)  # MobileNet also 224\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    if model_name == 'vgg':\n",
        "        img_array = preprocess_vgg(img_array)\n",
        "    elif model_name in ['resnet', 'resnet101']:\n",
        "        img_array = preprocess_resnet50(img_array) if model_name == 'resnet' else preprocess_resnet101(img_array)\n",
        "    elif model_name == 'mobile':\n",
        "        img_array = preprocess_mobile(img_array)\n",
        "    elif model_name == 'alex':\n",
        "        img_array /= 255.0  # Basic normalization for custom AlexNet\n",
        "    return img_array\n",
        "\n",
        "#                              Making Predictions\n",
        "def make_top_5_predictions(model, img_array, model_name):\n",
        "    preds = model.predict(img_array)\n",
        "    if model_name == 'vgg':\n",
        "        return decode_vgg(preds, top=5)\n",
        "    elif model_name == 'resnet':\n",
        "        return decode_resnet50(preds, top=5)\n",
        "    elif model_name == 'resnet101':\n",
        "        return decode_resnet101(preds, top=5)\n",
        "    elif model_name == 'mobile':\n",
        "        return decode_mobile(preds, top=5)\n",
        "    elif model_name == 'alex':\n",
        "        return decode_vgg(preds, top=5)  # Reuse VGG decoder as fallback (ImageNet classes)\n",
        "    return None\n",
        "\n",
        "def print_top_5_predictions(model, img_path, model_name):\n",
        "    img_array = load_and_preprocess_image(img_path, model_name)\n",
        "    predictions = make_top_5_predictions(model, img_array, model_name)\n",
        "    print(f\"Top 5 predictions for {model_name.upper()}:\")\n",
        "    for i, (imagenet_id, label, prob) in enumerate(predictions[0]):\n",
        "        print(f\"{i+1}. {label} ({prob*100:.2f}% probability)\")\n",
        "\n",
        "#                              Loading Pretrained Models\n",
        "alex_model = create_alexnet()  # Not pretrained; train or load weights separately\n",
        "resnet101_model = ResNet101(weights='imagenet')\n",
        "mobile_model = MobileNet(weights='imagenet')\n",
        "\n",
        "# Define image path\n",
        "img_path = r\"cat.png\"  # Or your Colab path\n",
        "\n",
        "# Get predictions\n",
        "print_top_5_predictions(alex_model, img_path, 'alex')\n",
        "print_top_5_predictions(resnet101_model, img_path, 'resnet101')\n",
        "print_top_5_predictions(mobile_model, img_path, 'mobile')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KySm1DiwR4mw",
        "outputId": "1d59d409-bf19-4a06-97d4-19074666c669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
            "Top 5 predictions for ALEX:\n",
            "1. electric_fan (0.11% probability)\n",
            "2. cassette_player (0.11% probability)\n",
            "3. beach_wagon (0.11% probability)\n",
            "4. rotisserie (0.11% probability)\n",
            "5. hip (0.11% probability)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x793588dbd800> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "Top 5 predictions for RESNET101:\n",
            "1. tabby (35.45% probability)\n",
            "2. tiger_cat (19.56% probability)\n",
            "3. hamper (6.27% probability)\n",
            "4. printer (2.00% probability)\n",
            "5. screen (1.65% probability)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x793588dbefc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675ms/step\n",
            "Top 5 predictions for MOBILE:\n",
            "1. tiger_cat (48.75% probability)\n",
            "2. tabby (41.32% probability)\n",
            "3. Egyptian_cat (6.21% probability)\n",
            "4. window_screen (0.55% probability)\n",
            "5. Band_Aid (0.34% probability)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Task 2: Transfer Learning***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W7u8sVHPTB7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I uploaded autism kaggle dataset in my drive mounted it and gave its path in code. The results showed both ResNet50 & VGG16 gave 80.67 test accuracy. The total runtime was 21min"
      ],
      "metadata": {
        "id": "rBYSDUZUn1Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 Import Libraries\n",
        "# Similar to tutorial, but added Sequential for building the model and ImageDataGenerator for dataset loading\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_vgg\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as preprocess_resnet\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Step 2 Load and Preprocess the Dataset\n",
        "# Adapted from single image to batch loading for the entire dataset using generators.\n",
        "# Assumes preprocessing specific to each model, like in the tutorial.\n",
        "def create_datagen(preprocess_func):\n",
        "    return ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_func,  # Model-specific preprocessing\n",
        "        rotation_range=20,  # Optional data augmentation for better generalization\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.0  # If no separate valid folder, you can split here; otherwise, use separate generators\n",
        "    )\n",
        "\n",
        "# Setting datast path\n",
        "data_dir = r'/content/drive/MyDrive/autism_dataset_kaggle'\n",
        "\n",
        "# Assuming standard structure; adjust if yours differs\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "valid_dir = os.path.join(data_dir, 'valid')\n",
        "\n",
        "batch_size = 32\n",
        "target_size = (224, 224)  # Matches tutorial's image size\n",
        "\n",
        "# For VGG16 (uses its preprocess)\n",
        "train_gen_vgg = create_datagen(preprocess_vgg).flow_from_directory(\n",
        "    train_dir, target_size=target_size, batch_size=batch_size, class_mode='binary'\n",
        ")\n",
        "valid_gen_vgg = create_datagen(preprocess_vgg).flow_from_directory(\n",
        "    valid_dir, target_size=target_size, batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "test_gen_vgg = create_datagen(preprocess_vgg).flow_from_directory(\n",
        "    test_dir, target_size=target_size, batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "\n",
        "# For ResNet50 (uses its preprocess)\n",
        "train_gen_resnet = create_datagen(preprocess_resnet).flow_from_directory(\n",
        "    train_dir, target_size=target_size, batch_size=batch_size, class_mode='binary'\n",
        ")\n",
        "valid_gen_resnet = create_datagen(preprocess_resnet).flow_from_directory(\n",
        "    valid_dir, target_size=target_size, batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "test_gen_resnet = create_datagen(preprocess_resnet).flow_from_directory(\n",
        "    test_dir, target_size=target_size, batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "\n",
        "# Print class mapping (0: autistic, 1: non_autistic – check your folder order)\n",
        "print(\"Class indices:\", train_gen_vgg.class_indices)\n",
        "\n",
        "# Step 3: Load Pretrained Models\n",
        "# (Similar to tutorial, but set include_top=False to remove ImageNet classifier head)\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base layers (transfer learning: only train the new head initially)\n",
        "vgg_base.trainable = False\n",
        "resnet_base.trainable = False\n",
        "\n",
        "# Step 4: Build Models with Custom Head for Binary Classification\n",
        "# (Adapted from prediction function; now we build a full model for training)\n",
        "def build_model(base_model):\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),  # To prevent overfitting\n",
        "        Dense(1, activation='sigmoid')  # Binary output (autistic vs. non_autistic)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "vgg_model = build_model(vgg_base)\n",
        "resnet_model = build_model(resnet_base)\n",
        "\n",
        "# Step 5: Define Dataset Paths\n",
        "# (Already done above; this is where you set data_dir)\n",
        "\n",
        "# Step 6: Train (Fine-Tune) and Evaluate the Models\n",
        "# (Adapted from getting predictions; now we train and test instead of just predicting)\n",
        "epochs = 10  # Adjust as needed; start small to test\n",
        "\n",
        "print(\"Training VGG16 model...\")\n",
        "vgg_history = vgg_model.fit(\n",
        "    train_gen_vgg,\n",
        "    validation_data=valid_gen_vgg,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "print(\"Evaluating VGG16 on test set...\")\n",
        "vgg_test_loss, vgg_test_acc = vgg_model.evaluate(test_gen_vgg)\n",
        "print(f\"VGG16 Test Accuracy: {vgg_test_acc * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining ResNet50 model...\")\n",
        "resnet_history = resnet_model.fit(\n",
        "    train_gen_resnet,\n",
        "    validation_data=valid_gen_resnet,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "print(\"Evaluating ResNet50 on test set...\")\n",
        "resnet_test_loss, resnet_test_acc = resnet_model.evaluate(test_gen_resnet)\n",
        "print(f\"ResNet50 Test Accuracy: {resnet_test_acc * 100:.2f}%\")\n",
        "\n",
        "# Optional: Interpret results (e.g., predict on a test batch)\n",
        "# Get a batch from test_gen_vgg\n",
        "test_images, test_labels = next(test_gen_vgg)\n",
        "vgg_preds = vgg_model.predict(test_images)\n",
        "# Print predictions (threshold 0.5 for binary)\n",
        "print(\"Sample VGG16 predictions (0: autistic, 1: non_autistic):\", (vgg_preds > 0.5).astype(int).flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hqPrnkYP-N9",
        "outputId": "c396d697-f99c-41af-86e4-9c1a1f7a4bec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2536 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n",
            "Found 300 images belonging to 2 classes.\n",
            "Found 2536 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n",
            "Found 300 images belonging to 2 classes.\n",
            "Class indices: {'autistic': 0, 'non_autistic': 1}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training VGG16 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 8s/step - accuracy: 0.6279 - loss: 5.4979 - val_accuracy: 0.7900 - val_loss: 0.5117\n",
            "Epoch 2/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 556ms/step - accuracy: 0.6888 - loss: 0.6582 - val_accuracy: 0.7800 - val_loss: 0.5203\n",
            "Epoch 3/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 551ms/step - accuracy: 0.7197 - loss: 0.5704 - val_accuracy: 0.7400 - val_loss: 0.4862\n",
            "Epoch 4/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 541ms/step - accuracy: 0.7450 - loss: 0.5570 - val_accuracy: 0.7600 - val_loss: 0.4740\n",
            "Epoch 5/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 569ms/step - accuracy: 0.7486 - loss: 0.5405 - val_accuracy: 0.8000 - val_loss: 0.4480\n",
            "Epoch 6/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 546ms/step - accuracy: 0.7687 - loss: 0.5093 - val_accuracy: 0.8000 - val_loss: 0.4973\n",
            "Epoch 7/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 539ms/step - accuracy: 0.7778 - loss: 0.4663 - val_accuracy: 0.7300 - val_loss: 0.4906\n",
            "Epoch 8/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 538ms/step - accuracy: 0.7657 - loss: 0.4937 - val_accuracy: 0.7800 - val_loss: 0.4789\n",
            "Epoch 9/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 550ms/step - accuracy: 0.7832 - loss: 0.4725 - val_accuracy: 0.7800 - val_loss: 0.5248\n",
            "Epoch 10/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 535ms/step - accuracy: 0.7767 - loss: 0.4595 - val_accuracy: 0.7600 - val_loss: 0.4631\n",
            "Evaluating VGG16 on test set...\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 12s/step - accuracy: 0.8064 - loss: 0.4180\n",
            "VGG16 Test Accuracy: 80.67%\n",
            "\n",
            "Training ResNet50 model...\n",
            "Epoch 1/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 672ms/step - accuracy: 0.6190 - loss: 8.7571 - val_accuracy: 0.6800 - val_loss: 0.6103\n",
            "Epoch 2/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 518ms/step - accuracy: 0.7205 - loss: 0.5623 - val_accuracy: 0.7000 - val_loss: 0.5651\n",
            "Epoch 3/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 514ms/step - accuracy: 0.7307 - loss: 0.5170 - val_accuracy: 0.7700 - val_loss: 0.5273\n",
            "Epoch 4/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 507ms/step - accuracy: 0.7539 - loss: 0.4705 - val_accuracy: 0.7800 - val_loss: 0.4910\n",
            "Epoch 5/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 518ms/step - accuracy: 0.7689 - loss: 0.5170 - val_accuracy: 0.7400 - val_loss: 0.5173\n",
            "Epoch 6/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 523ms/step - accuracy: 0.7657 - loss: 0.4962 - val_accuracy: 0.7700 - val_loss: 0.5213\n",
            "Epoch 7/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 540ms/step - accuracy: 0.7788 - loss: 0.4655 - val_accuracy: 0.7700 - val_loss: 0.4548\n",
            "Epoch 8/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 511ms/step - accuracy: 0.7720 - loss: 0.4581 - val_accuracy: 0.7700 - val_loss: 0.4571\n",
            "Epoch 9/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 517ms/step - accuracy: 0.7788 - loss: 0.4368 - val_accuracy: 0.7900 - val_loss: 0.4430\n",
            "Epoch 10/10\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 514ms/step - accuracy: 0.8105 - loss: 0.4276 - val_accuracy: 0.7200 - val_loss: 0.4522\n",
            "Evaluating ResNet50 on test set...\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 836ms/step - accuracy: 0.7552 - loss: 0.4903\n",
            "ResNet50 Test Accuracy: 80.67%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 974ms/step\n",
            "Sample VGG16 predictions (0: autistic, 1: non_autistic): [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    }
  ]
}